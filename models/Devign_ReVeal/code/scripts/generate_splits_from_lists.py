#!/usr/bin/env python3
"""Generate ``splits.csv`` files from original split text files.

This script lets us preserve an upstream train/valid/test split that was
materialized as ``split_train.txt``/``split_val.txt``/``split_test.txt`` when
exporting data with ``db2cfiles.py``. It enumerates the line-level GGNN dataset
(JSON Lines) to determine each example's zero-based index and then emits
``splits.csv`` files—mirroring :mod:`split_data_original`—by placing the
consolidated mapping inside ``v{fold}`` subdirectories under the requested
output directory.
"""
from __future__ import annotations

import argparse
import csv
from pathlib import Path
from typing import Dict, Iterable, List

import jsonlines


def read_split_list(path: Path) -> List[str]:
    """Return all non-empty file names listed in *path*.

    The text files generated by :mod:`db2cfiles` contain one file name per line
    with optional surrounding whitespace.  We normalise that whitespace here and
    ignore blank lines so the caller doesn't have to perform additional
    sanitisation.
    """

    names: List[str] = []
    with path.open("r", encoding="utf-8") as handle:
        for line_number, raw_line in enumerate(handle, start=1):
            name = raw_line.strip()
            if not name:
                continue
            names.append(name)
    return names


def build_index_map(dataset_path: Path) -> Dict[str, int]:
    """Map each ``file_name`` in *dataset_path* to its zero-based index."""

    index: Dict[str, int] = {}
    duplicates: List[str] = []
    with jsonlines.open(dataset_path) as reader:
        for idx, record in enumerate(reader):
            try:
                file_name = record["file_name"]
            except KeyError as exc:  # pragma: no cover - defensive check
                raise KeyError(
                    f"Missing 'file_name' field in record {idx} of {dataset_path}"
                ) from exc
            if not isinstance(file_name, str):
                raise TypeError(
                    f"Expected string 'file_name' in record {idx}, got {type(file_name)!r}"
                )
            normalized = file_name.strip()
            if normalized in index:
                duplicates.append(normalized)
            index[normalized] = idx
    if duplicates:
        dup_list = ", ".join(sorted(set(duplicates)))
        raise ValueError(
            "Encountered duplicate file names in dataset: "
            f"{dup_list}. File names must be unique to build splits."
        )
    return index


def iter_split_rows(
    mapping: Dict[str, int],
    assignments: Dict[str, Iterable[str]],
) -> Iterable[tuple[int, str]]:
    """Yield ``(index, split)`` rows for ``splits.csv``.

    Parameters
    ----------
    mapping:
        ``file_name`` → ``index`` lookup produced by :func:`build_index_map`.
    assignments:
        Dictionary mapping split label (``train``/``valid``/``test``) to the
        collection of file names that belong to that split.
    """

    seen: Dict[str, str] = {}
    for split_label, file_names in assignments.items():
        for name in file_names:
            if name not in mapping:
                raise KeyError(
                    f"File '{name}' from split '{split_label}' is missing in "
                    f"the dataset ({len(mapping)} examples)."
                )
            if name in seen:
                raise ValueError(
                    f"File '{name}' appears in both '{seen[name]}' and "
                    f"'{split_label}' splits. Each example must be unique."
                )
            seen[name] = split_label
            yield mapping[name], split_label

    missing = sorted(set(mapping) - set(seen))
    if missing:
        raise ValueError(
            "The following dataset entries were not assigned to any split: "
            + ", ".join(missing[:10])
            + (" ..." if len(missing) > 10 else "")
        )


def write_csv(output_path: Path, rows: Iterable[tuple[int, str]]) -> int:
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with output_path.open("w", encoding="utf-8", newline="") as handle:
        writer = csv.writer(handle)
        writer.writerow(["index", "split"])
        count = 0
        for idx, split in sorted(rows):
            writer.writerow([idx, split])
            count += 1
    return count



def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description=(
            "Create splits.csv by mapping file names listed in split_* text "
            "files to indices in a line-level GGNN dataset."
        )
    )
    parser.add_argument(
        "--dataset",
        required=True,
        type=Path,
        help=(
            "Path to the <project>-line-ggnn.jsonlines file (or any JSONL "
            "containing 'file_name' entries)."
        ),
    )
    parser.add_argument(
        "--split-dir",
        required=True,
        type=Path,
        help=(
            "Directory containing split_train.txt, split_val.txt, and "
            "split_test.txt generated during the export step."
        ),
    )
    parser.add_argument(
        "--output-dir",
        required=True,
        type=Path,
        help=(
            "Base directory for fold subdirectories (v1/, v2/, …) that will "
            "receive splits.csv files."
        ),
    )
    parser.add_argument(
        "--repeat-count",
        type=int,
        default=1,
        help=(
            "Number of sequential fold directories to populate, starting at "
            "--start-fold (defaults to 1)."
        ),
    )
    parser.add_argument(
        "--start-fold",
        type=int,
        default=1,
        help="Fold number to use for the first generated directory (default: 1).",
    )
    return parser.parse_args()



def main() -> None:
    args = parse_args()

    dataset_path = args.dataset
    split_dir = args.split_dir
    output_dir = args.output_dir
    repeat_count = args.repeat_count
    start_fold = args.start_fold

    if repeat_count <= 0:
        raise ValueError("--repeat-count must be a positive integer")
    if start_fold <= 0:
        raise ValueError("--start-fold must be a positive integer")

    assignments = {
        "train": read_split_list(split_dir / "split_train.txt"),
        "valid": read_split_list(split_dir / "split_val.txt"),
        "test": read_split_list(split_dir / "split_test.txt"),
    }

    index_map = build_index_map(dataset_path)
    rows = list(iter_split_rows(index_map, assignments))

    for offset in range(repeat_count):
        fold_number = start_fold + offset
        fold_dir = output_dir / f"v{fold_number}"
        csv_path = fold_dir / "splits.csv"
        count = write_csv(csv_path, rows)
        print(f"Wrote {count} rows to {csv_path}")


if __name__ == "__main__":
    main()
